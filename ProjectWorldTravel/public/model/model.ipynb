{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load data\n",
    "file_path = '../data/test_cases.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Ensure correct column names\n",
    "X = df['Input_Sentence']\n",
    "y_location = df['Expected_Destination']\n",
    "y_start_date = df['Expected_Start_Date']\n",
    "y_end_date = df['Expected_End_Date']\n",
    "y_num_people = df['Expected_Number_of_People']\n",
    "\n",
    "# Encode labels\n",
    "label_encoders = {}\n",
    "for col in ['Expected_Destination', 'Expected_Start_Date', 'Expected_End_Date', 'Expected_Number_of_People']:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_location_train, y_location_test = train_test_split(X, df['Expected_Destination'], test_size=0.2, random_state=42)\n",
    "_, _, y_start_date_train, y_start_date_test = train_test_split(X, df['Expected_Start_Date'], test_size=0.2, random_state=42)\n",
    "_, _, y_end_date_train, y_end_date_test = train_test_split(X, df['Expected_End_Date'], test_size=0.2, random_state=42)\n",
    "_, _, y_num_people_train, y_num_people_test = train_test_split(X, df['Expected_Number_of_People'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "\n",
    "# Determine the max sequence length\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "max_sequence_length = max(len(seq) for seq in X_train_seq)\n",
    "X_train_seq = pad_sequences(X_train_seq, padding='post', maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 624s 311ms/step - loss: 2.9641 - accuracy: 0.2061 - val_loss: 1.3687 - val_accuracy: 0.5614\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 581s 290ms/step - loss: 0.6874 - accuracy: 0.8049 - val_loss: 0.1663 - val_accuracy: 0.9748\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 547s 273ms/step - loss: 0.1667 - accuracy: 0.9655 - val_loss: 0.0681 - val_accuracy: 0.9869\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 506s 253ms/step - loss: 0.1430 - accuracy: 0.9640 - val_loss: 0.0328 - val_accuracy: 0.9960\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 539s 269ms/step - loss: 0.0652 - accuracy: 0.9860 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 726s 363ms/step - loss: 0.0506 - accuracy: 0.9876 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 749s 374ms/step - loss: 0.0065 - accuracy: 0.9996 - val_loss: 8.2276e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 782s 391ms/step - loss: 0.0592 - accuracy: 0.9857 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 800s 400ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.7724e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 655s 328ms/step - loss: 0.0573 - accuracy: 0.9861 - val_loss: 9.6435e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 687s 344ms/step - loss: 0.0038 - accuracy: 0.9998 - val_loss: 2.5672e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 2746s 1s/step - loss: 0.0407 - accuracy: 0.9900 - val_loss: 6.3951e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "2000/2000 [==============================] - 572s 286ms/step - loss: 0.0123 - accuracy: 0.9973 - val_loss: 3.2921e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "2000/2000 [==============================] - 710s 355ms/step - loss: 0.0145 - accuracy: 0.9962 - val_loss: 0.0147 - val_accuracy: 0.9957\n",
      "Epoch 1: Validation Accuracy = 0.5613750219345093\n",
      "Epoch 2: Validation Accuracy = 0.9748125076293945\n",
      "Epoch 3: Validation Accuracy = 0.9868749976158142\n",
      "Epoch 4: Validation Accuracy = 0.9959999918937683\n",
      "Epoch 5: Validation Accuracy = 1.0\n",
      "Epoch 6: Validation Accuracy = 1.0\n",
      "Epoch 7: Validation Accuracy = 1.0\n",
      "Epoch 8: Validation Accuracy = 1.0\n",
      "Epoch 9: Validation Accuracy = 1.0\n",
      "Epoch 10: Validation Accuracy = 1.0\n",
      "Epoch 11: Validation Accuracy = 1.0\n",
      "Epoch 12: Validation Accuracy = 1.0\n",
      "Epoch 13: Validation Accuracy = 1.0\n",
      "Epoch 14: Validation Accuracy = 0.9956874847412109\n",
      "625/625 [==============================] - 52s 83ms/step - loss: 2.5356e-04 - accuracy: 1.0000\n",
      "Location Model Test Accuracy: 1.0\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 804s 400ms/step - loss: 3.9697 - accuracy: 0.1905 - val_loss: 1.6385 - val_accuracy: 0.5714\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 875s 438ms/step - loss: 0.9454 - accuracy: 0.7826 - val_loss: 0.3508 - val_accuracy: 0.9574\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 881s 441ms/step - loss: 0.2872 - accuracy: 0.9583 - val_loss: 0.0599 - val_accuracy: 0.9977\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 894s 447ms/step - loss: 0.1644 - accuracy: 0.9708 - val_loss: 0.0295 - val_accuracy: 0.9961\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 844s 422ms/step - loss: 0.0908 - accuracy: 0.9830 - val_loss: 0.0450 - val_accuracy: 0.9901\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 740s 370ms/step - loss: 0.0738 - accuracy: 0.9847 - val_loss: 0.0155 - val_accuracy: 0.9975\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 727s 363ms/step - loss: 0.0904 - accuracy: 0.9785 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 666s 333ms/step - loss: 0.0490 - accuracy: 0.9889 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 694s 347ms/step - loss: 0.0605 - accuracy: 0.9849 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 818s 409ms/step - loss: 0.0379 - accuracy: 0.9908 - val_loss: 0.1638 - val_accuracy: 0.9561\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 756s 378ms/step - loss: 0.0409 - accuracy: 0.9903 - val_loss: 0.2526 - val_accuracy: 0.9354\n",
      "Epoch 1: Validation Accuracy = 0.5714374780654907\n",
      "Epoch 2: Validation Accuracy = 0.9574375152587891\n",
      "Epoch 3: Validation Accuracy = 0.9976875185966492\n",
      "Epoch 4: Validation Accuracy = 0.9960625171661377\n",
      "Epoch 5: Validation Accuracy = 0.9901250004768372\n",
      "Epoch 6: Validation Accuracy = 0.9975000023841858\n",
      "Epoch 7: Validation Accuracy = 1.0\n",
      "Epoch 8: Validation Accuracy = 1.0\n",
      "Epoch 9: Validation Accuracy = 1.0\n",
      "Epoch 10: Validation Accuracy = 0.956125020980835\n",
      "Epoch 11: Validation Accuracy = 0.9354375004768372\n",
      "625/625 [==============================] - 48s 77ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Start Date Model Test Accuracy: 1.0\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 921s 459ms/step - loss: 5.7715 - accuracy: 0.0386 - val_loss: 4.5360 - val_accuracy: 0.1879\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 820s 410ms/step - loss: 3.9285 - accuracy: 0.3450 - val_loss: 3.4160 - val_accuracy: 0.4920\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 823s 412ms/step - loss: 3.3416 - accuracy: 0.4947 - val_loss: 3.2221 - val_accuracy: 0.5174\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 1189s 595ms/step - loss: 3.2211 - accuracy: 0.5138 - val_loss: 3.1588 - val_accuracy: 0.5245\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 780s 390ms/step - loss: 3.1701 - accuracy: 0.5208 - val_loss: 3.1476 - val_accuracy: 0.5247\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 790s 395ms/step - loss: 3.1877 - accuracy: 0.5118 - val_loss: 3.1441 - val_accuracy: 0.5259\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 1999s 1000ms/step - loss: 3.1622 - accuracy: 0.5187 - val_loss: 3.1428 - val_accuracy: 0.5255\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 839s 420ms/step - loss: 3.1578 - accuracy: 0.5189 - val_loss: 3.1435 - val_accuracy: 0.5254\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 1201s 600ms/step - loss: 3.1530 - accuracy: 0.5203 - val_loss: 3.1497 - val_accuracy: 0.5245\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 1148s 574ms/step - loss: 3.1269 - accuracy: 0.5267 - val_loss: 3.1413 - val_accuracy: 0.5255\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 1122s 561ms/step - loss: 3.1621 - accuracy: 0.5169 - val_loss: 3.1420 - val_accuracy: 0.5254\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 1120s 560ms/step - loss: 3.1479 - accuracy: 0.5212 - val_loss: 3.1443 - val_accuracy: 0.5250\n",
      "Epoch 13/20\n",
      "2000/2000 [==============================] - 1489s 745ms/step - loss: 3.1414 - accuracy: 0.5223 - val_loss: 3.1439 - val_accuracy: 0.5251\n",
      "Epoch 1: Validation Accuracy = 0.18787500262260437\n",
      "Epoch 2: Validation Accuracy = 0.492000013589859\n",
      "Epoch 3: Validation Accuracy = 0.5174375176429749\n",
      "Epoch 4: Validation Accuracy = 0.5245000123977661\n",
      "Epoch 5: Validation Accuracy = 0.5247499942779541\n",
      "Epoch 6: Validation Accuracy = 0.5258749723434448\n",
      "Epoch 7: Validation Accuracy = 0.5254999995231628\n",
      "Epoch 8: Validation Accuracy = 0.5253750085830688\n",
      "Epoch 9: Validation Accuracy = 0.5245000123977661\n",
      "Epoch 10: Validation Accuracy = 0.5254999995231628\n",
      "Epoch 11: Validation Accuracy = 0.5254374742507935\n",
      "Epoch 12: Validation Accuracy = 0.5249999761581421\n",
      "Epoch 13: Validation Accuracy = 0.5251250267028809\n",
      "625/625 [==============================] - 50s 80ms/step - loss: 3.1763 - accuracy: 0.5199\n",
      "End Date Model Test Accuracy: 0.5199499726295471\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 1250s 624ms/step - loss: 0.2865 - accuracy: 0.9179 - val_loss: 0.0784 - val_accuracy: 0.9761\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 864s 432ms/step - loss: 0.0776 - accuracy: 0.9762 - val_loss: 0.0779 - val_accuracy: 0.9762\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 913s 456ms/step - loss: 0.0772 - accuracy: 0.9763 - val_loss: 0.0777 - val_accuracy: 0.9756\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 713s 356ms/step - loss: 0.0771 - accuracy: 0.9763 - val_loss: 0.0777 - val_accuracy: 0.9762\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 611s 305ms/step - loss: 0.0798 - accuracy: 0.9757 - val_loss: 0.0777 - val_accuracy: 0.9762\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 600s 300ms/step - loss: 0.0771 - accuracy: 0.9762 - val_loss: 0.0777 - val_accuracy: 0.9756\n",
      "Epoch 1: Validation Accuracy = 0.9761250019073486\n",
      "Epoch 2: Validation Accuracy = 0.976187527179718\n",
      "Epoch 3: Validation Accuracy = 0.9756249785423279\n",
      "Epoch 4: Validation Accuracy = 0.976187527179718\n",
      "Epoch 5: Validation Accuracy = 0.976187527179718\n",
      "Epoch 6: Validation Accuracy = 0.9756249785423279\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.0829 - accuracy: 0.9742\n",
      "Number of People Model Test Accuracy: 0.9741500020027161\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, padding='post', maxlen=max_sequence_length)\n",
    "\n",
    "X_test_seq = preprocess_text(X_test)\n",
    "\n",
    "# Define the LSTM model\n",
    "def create_lstm_model(output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_sequence_length))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "def train_and_evaluate(X_train_seq, y_train, X_test_seq, y_test, output_dim, model_name):\n",
    "    model = create_lstm_model(output_dim)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    history = model.fit(X_train_seq, y_train, validation_split=0.2, epochs=20, batch_size=32, callbacks=[early_stopping])\n",
    "    \n",
    "    # Print validation accuracy for each epoch\n",
    "    for epoch, val_acc in enumerate(history.history['val_accuracy']):\n",
    "        print(f'Epoch {epoch + 1}: Validation Accuracy = {val_acc}')\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test_seq, y_test)\n",
    "    print(f'{model_name} Model Test Accuracy: {accuracy}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train models for each target variable\n",
    "location_model = train_and_evaluate(X_train_seq, y_location_train, X_test_seq, y_location_test, len(label_encoders['Expected_Destination'].classes_), 'Location')\n",
    "start_date_model = train_and_evaluate(X_train_seq, y_start_date_train, X_test_seq, y_start_date_test, len(label_encoders['Expected_Start_Date'].classes_), 'Start Date')\n",
    "end_date_model = train_and_evaluate(X_train_seq, y_end_date_train, X_test_seq, y_end_date_test, len(label_encoders['Expected_End_Date'].classes_), 'End Date')\n",
    "num_people_model = train_and_evaluate(X_train_seq, y_num_people_train, X_test_seq, y_num_people_test, len(label_encoders['Expected_Number_of_People'].classes_), 'Number of People')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_model.save('location_model.h5')\n",
    "start_date_model.save('start_date_model.h5')\n",
    "end_date_model.save('end_date_model.h5')\n",
    "num_people_model.save('num_people_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 538ms/step\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 485ms/step\n",
      "1/1 [==============================] - 1s 516ms/step\n",
      "Location:  Cambridge\n",
      "Start Date:  3/19\n",
      "End Date:  October 5th\n",
      "Number of People: 3\n"
     ]
    }
   ],
   "source": [
    "# Load and use the models\n",
    "location_model = tf.keras.models.load_model('location_model.h5')\n",
    "start_date_model = tf.keras.models.load_model('start_date_model.h5')\n",
    "end_date_model = tf.keras.models.load_model('end_date_model.h5')\n",
    "num_people_model = tf.keras.models.load_model('num_people_model.h5')\n",
    "\n",
    "# Example prediction\n",
    "input_sentence = \"Let's have an adventure to Cambridge from 3/19 to October 5th with 3 buddies\"\n",
    "input_seq = preprocess_text([input_sentence])\n",
    "\n",
    "location_pred = np.argmax(location_model.predict(input_seq), axis=-1)[0]\n",
    "start_date_pred = np.argmax(start_date_model.predict(input_seq), axis=-1)[0]\n",
    "end_date_pred = np.argmax(end_date_model.predict(input_seq), axis=-1)[0]\n",
    "num_people_pred = np.argmax(num_people_model.predict(input_seq), axis=-1)[0]\n",
    "\n",
    "location = label_encoders['Expected_Destination'].inverse_transform([location_pred])[0]\n",
    "start_date = label_encoders['Expected_Start_Date'].inverse_transform([start_date_pred])[0]\n",
    "end_date = label_encoders['Expected_End_Date'].inverse_transform([end_date_pred])[0]\n",
    "num_people = label_encoders['Expected_Number_of_People'].inverse_transform([num_people_pred])[0]\n",
    "\n",
    "print(f\"Location: {location}\")\n",
    "print(f\"Start Date: {start_date}\")\n",
    "print(f\"End Date: {end_date}\")\n",
    "print(f\"Number of People: {num_people}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 651ms/step\n",
      "1/1 [==============================] - 1s 543ms/step\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 1s 667ms/step\n",
      "Location:  Cambridge\n",
      "Start Date:  3/19\n",
      "End Date:  October 5th\n",
      "Number of People: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define max sequence length as per the model's expected input\n",
    "max_sequence_length = 22\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(texts, tokenizer):\n",
    "    # Tokenize the text\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # Pad the sequences\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "    return padded_sequences\n",
    "\n",
    "# Load models\n",
    "location_model = tf.keras.models.load_model('location_model.h5')\n",
    "start_date_model = tf.keras.models.load_model('start_date_model.h5')\n",
    "end_date_model = tf.keras.models.load_model('end_date_model.h5')\n",
    "num_people_model = tf.keras.models.load_model('num_people_model.h5')\n",
    "\n",
    "# Load the trained tokenizer\n",
    "# Replace 'tokenizer.pkl' with the actual path to your saved tokenizer\n",
    "\n",
    "\n",
    "# Example prediction\n",
    "input_sentence = \"Let's have an adventure to Cambridge from 3/19 to October 5th with 3 buddies\"\n",
    "input_seq = preprocess_text([input_sentence], tokenizer)\n",
    "\n",
    "# Predict using the models\n",
    "location_pred = np.argmax(location_model.predict(input_seq), axis=-1)[0]\n",
    "start_date_pred = np.argmax(start_date_model.predict(input_seq), axis=-1)[0]\n",
    "end_date_pred = np.argmax(end_date_model.predict(input_seq), axis=-1)[0]\n",
    "num_people_pred = np.argmax(num_people_model.predict(input_seq), axis=-1)[0]\n",
    "\n",
    "location = label_encoders['Expected_Destination'].inverse_transform([location_pred])[0]\n",
    "start_date = label_encoders['Expected_Start_Date'].inverse_transform([start_date_pred])[0]\n",
    "end_date = label_encoders['Expected_End_Date'].inverse_transform([end_date_pred])[0]\n",
    "num_people = label_encoders['Expected_Number_of_People'].inverse_transform([num_people_pred])[0]\n",
    "\n",
    "print(f\"Location: {location}\")\n",
    "print(f\"Start Date: {start_date}\")\n",
    "print(f\"End Date: {end_date}\")\n",
    "print(f\"Number of People: {num_people}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
